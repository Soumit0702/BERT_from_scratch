{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCjaOGJTHTa-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Configurations"
      ],
      "metadata": {
        "id": "1KfjRxHQRbC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelArgs:\n",
        "    dim: int = 768\n",
        "    n_layers: int = 12\n",
        "    n_heads: int = 12\n",
        "    n_segment:int = 3\n",
        "    n_kv_heads: Optional[int] = None\n",
        "    multiple_of: int = 64\n",
        "    ffn_dim_multiplier: Optional[float] = 2.0\n",
        "    norm_eps: float = 1e-12\n",
        "    learning_rate: float = 5e-4\n",
        "    vocab_size: int = 30522\n",
        "    eval_iters: int = 100\n",
        "    qkv_bias: bool = False\n",
        "    block_size: int = 16\n",
        "    max_iters: int = 1000\n",
        "    eval_interval: int = 200\n",
        "    drop_out: float = 0.1\n",
        "    max_batch_size: int = 8\n",
        "    max_seq_len: int = 768\n",
        "    context_length: int = 12\n",
        "    num_experts: int = 4\n",
        "    top_k: int = 2\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n"
      ],
      "metadata": {
        "id": "cm5iY0aqHW9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert Embeddings"
      ],
      "metadata": {
        "id": "aQ8xXgbMRfBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTEmbedding(nn.Module):\n",
        "    def __init__(self,args:ModelArgs):\n",
        "        super().__init__()\n",
        "        self.tok_embed = nn.Embedding(args.vocab_size, args.dim)\n",
        "        self.seg_embed = nn.Embedding(args.n_segment, args.dim)\n",
        "        self.pos_embed = nn.Embedding(args.max_seq_len, args.dim)\n",
        "\n",
        "        self.drop = nn.Dropout(args.drop_out)\n",
        "        self.pos_inp = torch.tensor([i for i in range(args.max_seq_len)],)\n",
        "\n",
        "    def forward(self, seq, seg):\n",
        "        seg = seg.unsqueeze(1).expand(-1, seq.shape[0])\n",
        "\n",
        "        embed_val = self.tok_embed(seq) + self.seg_embed(seg) + self.pos_embed(self.pos_inp)\n",
        "        embed_val = self.drop(embed_val)\n",
        "        return embed_val"
      ],
      "metadata": {
        "id": "TXws64zTHW_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0):\n",
        "    assert head_dim % 2 == 0, \"Head dimension must be divisible by 2.\"\n",
        "    theta_numerator = torch.arange(0, head_dim, 2, device=device).float()\n",
        "    theta = 1.0 / (theta ** (theta_numerator / head_dim))\n",
        "    m = torch.arange(seq_len, device=device)\n",
        "    freqs = torch.outer(m, theta).float()\n",
        "    freqs_complex = torch.polar(torch.ones_like(freqs, device=device), freqs)\n",
        "\n",
        "    return freqs_complex\n"
      ],
      "metadata": {
        "id": "vPMLlYuzHXBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
        "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
        "    x_rotated = x_complex * freqs_complex\n",
        "    x_out = torch.view_as_real(x_rotated)\n",
        "    x_out = x_out.reshape(*x.shape).to(device)\n",
        "    return x_out.type_as(x)\n"
      ],
      "metadata": {
        "id": "aBqkTCQWHXDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
        "    if n_rep == 1:\n",
        "        return x\n",
        "    return (\n",
        "        x[:, :, :, None, :]\n",
        "        .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
        "        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "l7t3hDekHXFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Attention"
      ],
      "metadata": {
        "id": "aBvmdpqyRkgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.head_q = args.n_heads\n",
        "        self.head_kv = args.n_kv_heads if args.n_kv_heads is not None else args.n_heads\n",
        "        self.rep = self.head_q // self.head_kv\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "        self.W_q = nn.Linear(args.dim, self.head_dim * self.head_q, bias=args.qkv_bias)\n",
        "        self.W_k = nn.Linear(args.dim, self.head_dim * self.head_kv, bias=args.qkv_bias)\n",
        "        self.W_v = nn.Linear(args.dim, self.head_dim * self.head_kv, bias=args.qkv_bias)\n",
        "        self.out_proj = nn.Linear(args.dim, args.dim)\n",
        "\n",
        "        self.k_cache = torch.zeros(\n",
        "        args.max_batch_size, args.max_seq_len, self.head_kv, self.head_dim, device=args.device\n",
        "        )\n",
        "        self.v_cache = torch.zeros(\n",
        "        args.max_batch_size, args.max_seq_len, self.head_kv, self.head_dim, device=args.device\n",
        "        )\n",
        "\n",
        "        self.register_buffer(\n",
        "        'mask',\n",
        "        torch.triu(torch.ones(args.max_seq_len, args.max_seq_len, device=args.device), diagonal=1)\n",
        "        )\n",
        "\n",
        "        self.drop_out = nn.Dropout(args.drop_out)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int):\n",
        "        batch, seq_len, _ = x.shape\n",
        "\n",
        "        freqs_complex = precompute_theta_pos_frequencies(self.head_dim, seq_len, device=x.device)\n",
        "\n",
        "        query = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        query = query.view(batch, seq_len, self.head_q, self.head_dim)\n",
        "        k = k.view(batch, seq_len, self.head_kv, self.head_dim)\n",
        "        v = v.view(batch, seq_len, self.head_kv, self.head_dim)\n",
        "\n",
        "        query = apply_rotary_embeddings(query, freqs_complex, device=x.device)\n",
        "        k = apply_rotary_embeddings(k, freqs_complex, device=x.device)\n",
        "\n",
        "        self.k_cache[:batch, start_pos: start_pos + seq_len] = k\n",
        "        self.v_cache[:batch, start_pos: start_pos + seq_len] = v\n",
        "\n",
        "        key = self.k_cache[:batch, :start_pos + seq_len]\n",
        "        value = self.v_cache[:batch, :start_pos + seq_len]\n",
        "\n",
        "        key = repeat_kv(key, self.rep)\n",
        "        value = repeat_kv(value, self.rep)\n",
        "\n",
        "        query = query.transpose(1, 2)\n",
        "        key = key.transpose(1, 2)\n",
        "        value = value.transpose(1, 2)\n",
        "\n",
        "        attention_scores = query @ key.transpose(2, 3)\n",
        "\n",
        "        mask_bool = self.mask[:seq_len, :seq_len].bool()\n",
        "        attention_scores = attention_scores.masked_fill(mask_bool, float('-inf'))\n",
        "\n",
        "        attention_weights = F.softmax(attention_scores/key.shape[-1]**0.5,dim = -1)\n",
        "        attention_weights = self.drop_out(attention_weights)\n",
        "\n",
        "        context_vec = attention_weights @ value\n",
        "        context_vec = context_vec.transpose(1, 2).contiguous()\n",
        "        context_vec = context_vec.view(batch, seq_len, -1)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec\n"
      ],
      "metadata": {
        "id": "X5INEx4PHXII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Block"
      ],
      "metadata": {
        "id": "yGIL5lYpR2TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BertEncoderBlock(nn.Module):\n",
        "    def __init__(self,args:ModelArgs):\n",
        "        super(BertEncoderBlock, self).__init__()\n",
        "        self.hidden_size = args.dim\n",
        "        self.num_heads = args.n_heads\n",
        "        self.intermediate_size = 4*args.dim\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(args)\n",
        "\n",
        "        self.attention_norm = nn.LayerNorm(args.dim)\n",
        "        self.attention_dropout = nn.Dropout(args.drop_out)\n",
        "\n",
        "        self.intermediate_dense = nn.Linear(args.dim, 4*args.dim)\n",
        "        self.output_dense = nn.Linear(4*args.dim, args.dim)\n",
        "\n",
        "        self.activation = F.gelu\n",
        "\n",
        "        self.ffn_norm = nn.LayerNorm(args.dim)\n",
        "        self.ffn_dropout = nn.Dropout(args.drop_out)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        attention_output, _ = self.self_attention(x, x, x, attn_mask=attention_mask)\n",
        "        attention_output = self.attention_dropout(attention_output)\n",
        "        attention_output = self.attention_norm(x + attention_output)\n",
        "\n",
        "        intermediate_output = self.activation(self.intermediate_dense(attention_output))\n",
        "        ffn_output = self.output_dense(intermediate_output)\n",
        "        ffn_output = self.ffn_dropout(ffn_output)\n",
        "        output = self.ffn_norm(attention_output + ffn_output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "47Iqu6wtLc14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Layer"
      ],
      "metadata": {
        "id": "JcL74FgWR0AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BertEncoderLayer(nn.Module):\n",
        "    def __init__(self, args:ModelArgs):\n",
        "        super(BertEncoderLayer, self).__init__()\n",
        "        self.num_layers = args.n_layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            BertEncoderBlock(args)\n",
        "            for _ in range(args.n_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(args.dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask)\n",
        "        return self.layer_norm(x)\n"
      ],
      "metadata": {
        "id": "zuPhGXNOJ3cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "7LRfpE1pRwNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 args:ModelArgs):\n",
        "        super().__init__()\n",
        "        self.embedding = BERTEmbedding(args)\n",
        "        self.encoder_layer = BertEncoderLayer(args)\n",
        "        self.encoder_block = BertEncoderBlock(args)\n",
        "\n",
        "    def forward(self, seq, seg):\n",
        "        out = self.embedding(seq, seg)\n",
        "        out = self.encoder_block(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "BatUcaAcPdAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    args = ModelArgs()\n",
        "    args.device = device\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    model = BERT(ModelArgs())\n",
        "    model = model.to(device)\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcfz25TyN3rJ",
        "outputId": "0ba27dcb-d97e-47bd-b435-08721dbfa519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116.146944 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pYduA9qLHpLO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}